\chapter[Benchmarking, environment setup and characterization of the Performance Measurement Unit]{Benchmarking, environment setup and characterization of the Sandy Bridge Performance Measurement Unit}\label{chapter:envsetup}

This chapters aims to introduce the environment used in the execution of the developed solution and the benchmarks that will be used as supervised programs to test the functioning of such solution. The second part of this chapter aims to present the first experimental results that seek to explain how the setting of the sampling configuration influences the obtained samples.

\section{Testing environment}\label{section:runningenv}
The testing unit consists of a host with two NUMA nodes joined by a QPI link. Every node is an Intel Xeon E5 compliant to the Intel Sandy Bridge-EP microarchitecture. Every node contains 8 physical cores, where every core has a clock speed of 2.6 GHz. However, because of the Hyper threading available in this processor, the operating system sees 16 cores for every NUMA node, making it 32 cores in total. The host altogether has a RAM capacity of 128 GB. All the tests were performed under the 3.16 version of the Kernel.

\section{Employed Benchmarks}\label{section:emplybmchs}

\subsection{Distgen}\label{section:distgen}
The first algorithm to test aims to explore the behavior of the page movement functionality in a simple application which accesses a memory location. The simplest scenario would be to implement a loop that traverses a consecutive memory space sequentially and use the SPM tool to evaluate the performance of the developed algorithm. The objection to this idea is that because of the hardware prefetcher present in most of the modern processors, this application will execute very quickly. The first modification would be to access a random memory location in the memory space for each iteration, instead of the sequential access proposed first. The random location can be chosen by making use of the \textbf{random} call, present in the stock C library. The traversal executes slower than the sequential access as predicted, but the problem that was found is that the C random function has locks in its implementation that become a considerable important bottleneck.

Because of the implementation with locks of the C function, what is needed is a random number generator faster than the stock random function. It does not matter if the random generation is less strict, because this generation is only needed to avoid engaging the hardware prefetcher and for this purpose \textbf{Distgen} comes into play. Distgen, coded by Josef Weidendorfer, whose availability is presented in code listing \textit{original distgen}, is an algorithm that transverses a memory area in a sequential or pseudo random manner where the latter mode is executed with low overhead. The execution command for distgen is as follows:
\begin{center}
\texttt{distgen [-p] [n-iterations] [size]}
\end{center}

Where the presence of the -p indicates that the traversal will use the pseudo random sequence, when not present it will transverse the space in a streaming fashion. The optional parameter n-iterations determines how many times the space will be traversed and the size parameters determines the size of the domain to iterate through. If any of these parameters is not specified the default will be applied.

\subsection{LAMA}\label{section:Lama}

\section{Characterization of an Intel Sandy Brige Performance Measurement Unit}\label{section:pmu-charzn}

\subsection{Latency weight distribution}\label{subsection:pmu-latwei}

The sample latency distribution show the distribution of the latency values when the sample is taken under a certain period and minimum weight threshold, where every bar corresponds to a specific latency weight threshold and all the bars on the same picture correspond to the same sampling period. The weight of every sample is placed in its corresponding interval and every color corresponds to the percentages of samples belonging to that interval.  The results are shown in figures X and Y, which correspond to the periods of 20 and 200. The other figures are omitted because of an almost identical distribution to the ones shown

It can be observed how the working of the minimum weight threshold works: for all the distributions with a weight under 250, it can be seen how the samples with weights between 200 and 150, 250 and 300 and 300 and 350 dominate the distribution but with filtering values above that value the samples in those intervals no longer appear. 2.	The distribution of the samples is very similar throughout all the period, which shows that it is independent of the sampling period.3:	In this algorithm memory latency is an important problem, taking into account that local memory request are served with latencies of tens of clock cycles, at most.

\begin{figure}[th]
	\centering
		\includegraphics[width=.8\textwidth]{figures/latencydistw20.eps}
		\caption[Sample weight distribution with a period of 20 instructions per sample.]{Sample weight distribution with a period of 20 instructions per sample with different minimum weight thresholds. }
		\label{fig:latencydistw20}
\end{figure}

\begin{figure}[th]
	\centering
		\includegraphics[width=.8\textwidth]{figures/latencydistw200.eps}
		\caption[Sample weight distribution with a period of 200 instructions per sample.]{Sample weight distribution with a period of 20 instructions per sample with different minimum weight thresholds.}
		\label{fig:latencydistw200}
\end{figure}

\subsection{Number of obtained samples}\label{subsection:pmu-obtainedsamp}

The sample distributions ommit a very important type of information, which is the number of samples which are acquired for every period and weight threshold combination. Knowing how many samples is important, because a high number of samples helps the SPM tool form a good idea of the memory access behavior of the observed process, but on the other hand every sample received must be processed which in turn implies CPU consumption which could be taken away from the supervised process. 

Figure X shows how the number of samples taken reacts to changes in the sampling period and minimum weight threshold. Because a higher weight threshold implies discarding a bigger number of samples under a certain value this, relationship decreases. This experiments also confirms that a higher period is directly related to an increasing number of samples.


\begin{figure}[th]
	\centering
		\includegraphics[width=.8\textwidth]{figures/number-samples.eps}
		\caption{Number of samples obtained for different weight settings }
		\label{fig:pmu-obtainedsamp}
\end{figure}

\subsection{Frequency of page access}\label{subsection:pmu-freqpgacc}

Figure X shows how the number of page accesses are distributed in a distgen run, this means that for an execution run of the algorithm a certain percentage of the pages were accessed within the given interval, this information is highly dependent on the nature of the algorithm observed. It can be seen that most of the pages were accessed between 3 and 10 times. This information is of very essential importance because it is necessary to decide after how many accesses it is possible to consider a page a certain remote or local access.

\begin{figure}[th]
	\centering
		\includegraphics[width=.8\textwidth]{figures/sample-concentration.eps}
		\caption{Percentage of pages that were accessed a given number of times }
		\label{fig:pmu-concentsamp}
\end{figure}

\subsection{Completion time of the move pages call}\label{subsection:pmu-movpatime}


The move\_pages directive plays a very important role in the SPM tool because it provides the means to reallocate one page from one node to another, as well as querying in which node a given page, or pages reside in Figure XYZ shows the behavior of such call as function of the number of pages to move. Two scenarios are measured: the first one is the relocation of the pages in conditions of such process running exclusively on the system and the other is the same relocation together with a distgen process using all the cores, which represents a congestion scenarios. It can be seen that for a small number of pages this behavior is almost lineal but it meets an inflection point where the function completion time tends to grow smaller. The effects of the congestion can be seen since for most of the measurements the function takes longer to complete. In more elaborate scenarios as the ones shown here the function could take longer to complete, figures in the order of four seconds were measured with scenarios of redirections in multiple directions with sizes of 200 thousand samples.

\begin{figure}[th]  
	\centering
		\includegraphics[width=.8\textwidth]{figures/mov-pages-time.eps}
		\caption{Completion times for the move pages call for different page count under contention and no load scenarios}
		\label{fig:mov-pages-time.eps}
\end{figure}
